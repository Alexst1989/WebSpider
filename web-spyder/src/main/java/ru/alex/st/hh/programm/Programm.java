package ru.alex.st.hh.programm;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import ru.alex.st.hh.config.ConfigurationBuilder;
import ru.alex.st.hh.config.SpiderConfiguration;
import ru.alex.st.hh.web.spider.WebSpider;

public class Programm {

	private static final Logger LOGGER = LogManager.getLogger(Programm.class);


	/*
	 * Задание:
	 * 
	 * · Написать паука, который параллельно выкачивает все статьи википедии по
	 * ссылкам и сохраняет на диске.
	 * 
	 * · В параметре паука задаем входной url и глубину выкачивания.
	 * 
	 * · Плюс к этому сделать простой поиск по слову в скаченных статьях, без
	 * использования фреймворков полнотекстового поиска.
	 * 
	 * · Желательно решение близкое к production-quality: конфиги, логи, чтобы
	 * можно было анализировать ошибки в оффлайн и тп
	 * 
	 * Реализация:
	 * Выкачивание статей происходит параллельно. Ради синхронизации метода 
	 * WebSpider.loadPages() каждый родитель ждет завершения выкачивания дочерних ссылок.
	 * Выкачивание дочерних ссылок происходит в ThreadPool, отдельном от ThreadPool в котором выполняется родитель.
	 * После выкачивания дочерних ссылок ответственных за них ThreadPool завершает работу.
	 * Каждый ThreadPool ограничен по количеству потоков.
	 * Парсинг ссылок происходит по regex выражениям (класс LinkParser). Легко изменяется / расширяется.
	 * Учтены относительные ссылки. В них дописывается имя сервера и/или протокол.
	 * Уникальность выкачиваемых статей обрабатывается по уникальности ссылок (WebSpider.globalLinkSet), без анализа содержания статей, 
	 * что иногда приводит к дубликатам. Разные ссылки могут приводить к одной и той же статье.
	 * 
	 * Запись на диск осуществляется в древовидном виде с папками (для удобства проверок), что при достаточно большой глубине выкачивания
	 * может привести к слишком длинному пути файла. Можно легко переписать на хранение в одной папке.
	 * 
	 * Поиск по статьям реализован просто, в одном потоке, с ипользованием построенного дерева на этапе выкачивания,
	 * где есть вся информация о ссылках и путях к файлам. Поиск основан на regex выражениях, происходит построчно.
	 * 
	 * Не успел дописать сериализацию дерева на жесткий диск, чтобы поиск можно было осуществлять поиск 
	 * без обязательного предварительного вызова метода WebSpider.loadPages(). 
	 * Пока перед поиском нужно обязательно вызвать loadPages().
	 * Можно сделать поиск без привязки к TreeNode по всему содержимому папки с использованием DirectoryStream.
	 * 
	 * 
	 *  
	 */

	public static void main(String args[]) {
		SpiderConfiguration config = new ConfigurationBuilder()
	                    .setStartUrl("https://ru.wikipedia.org/wiki") //Входной url
		                .setDepth(7)                                  //Глубина выкачвания
						.setLinkLevelLimit(10)                        //0 без ограничений. Для отладочных целей можно ограничить число скачиваемых статей на каждом уровне
						.setDiskStoragePath("D:/develop/Temp")        //Путь для хранения скачанных статей
						.setLocale("en")                           //Для локализации приложения
						.build();
		
		WebSpider spider = new WebSpider(config); //Создаем экземпляр паука по конфиигу
		spider.loadPages();                       //Выкачивание статей (синхронный метод)
		
        LOGGER.info(spider.searchInLoadedPages("[рР]усский")); // Поиск по скачанным статьям, regex выражение
		
        
		
		LOGGER.info("Finished");
	}

}
